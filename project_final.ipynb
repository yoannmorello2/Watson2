{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5836ec62",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcff63",
   "metadata": {},
   "source": [
    "All cells from here to the next header must be executed before running any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b3a9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "438175eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['language']=='English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a319825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv')\n",
    "df_test = df_test[df_test['language']=='English'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edcc5b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fdcd1bd867</td>\n",
       "      <td>From Cockpit Country to St. Ann's Bay</td>\n",
       "      <td>From St. Ann's Bay to Cockpit Country.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7cfb3d272c</td>\n",
       "      <td>Look, it's your skin, but you're going to be i...</td>\n",
       "      <td>The boss will fire you if he sees you slacking...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>2b78e2a914</td>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>7e9943d152</td>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>5085923e6c</td>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>fc8e2fd1fe</td>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>44301dfb14</td>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                            premise  \\\n",
       "0      5130fd2cb5  and these comments were considered in formulat...   \n",
       "1      5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "3      5622f0c60b  you know they can't really defend themselves l...   \n",
       "7      fdcd1bd867              From Cockpit Country to St. Ann's Bay   \n",
       "8      7cfb3d272c  Look, it's your skin, but you're going to be i...   \n",
       "...           ...                                                ...   \n",
       "12115  2b78e2a914  The results of even the most well designed epi...   \n",
       "12116  7e9943d152  But there are two kinds of  the pleasure of do...   \n",
       "12117  5085923e6c  The important thing is to realize that it's wa...   \n",
       "12118  fc8e2fd1fe  At the west end is a detailed model of the who...   \n",
       "12119  44301dfb14  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis lang_abv language  \\\n",
       "0      The rules developed in the interim were put to...       en  English   \n",
       "1      Practice groups are not permitted to work on t...       en  English   \n",
       "3      They can't defend themselves because of their ...       en  English   \n",
       "7                 From St. Ann's Bay to Cockpit Country.       en  English   \n",
       "8      The boss will fire you if he sees you slacking...       en  English   \n",
       "...                                                  ...      ...      ...   \n",
       "12115  All studies have the same amount of uncertaint...       en  English   \n",
       "12116  But there are two kinds of the pleasure of doi...       en  English   \n",
       "12117                   It cannot be moved, now or ever.       en  English   \n",
       "12118       The model temple complex is at the east end.       en  English   \n",
       "12119      Ataturk was the father of the Turkish nation.       en  English   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          2  \n",
       "3          0  \n",
       "7          2  \n",
       "8          1  \n",
       "...      ...  \n",
       "12115      2  \n",
       "12116      0  \n",
       "12117      2  \n",
       "12118      2  \n",
       "12119      0  \n",
       "\n",
       "[6870 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f67addd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_target(t):\n",
    "    v=np.zeros(3,dtype='int')\n",
    "    v[t]=1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d1bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label']=df_train['label'].apply(one_hot_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "182bbc11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['premise'].str.split().apply(len).max()   #longest sequence of tokens in premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "616b5941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['hypothesis'].str.split().apply(len).max()   #longest sequence of tokens in hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ce0311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_train['premise'].str.split().apply(len)+df_train['hypothesis'].str.split().apply(len)).max() #longest premise+hyp sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3ec647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=216  #set max sequence len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fad7c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)*.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "300c1aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valuation dataset\n",
    "df_val = df_train.sample(687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0282fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train[~df_train.index.isin(df_val.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0539bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val[['premise','hypothesis','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b970e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['premise','hypothesis','label']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d24d1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bba34828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.optim.lr_scheduler import LinearLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19186f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
    "\n",
    "        self.data = data  # pandas dataframe\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent1 = str(self.data.loc[index, 'premise'])\n",
    "        sent2 = str(self.data.loc[index, 'hypothesis'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,  # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "        if bert_model == \"distilbert-base-uncased\":\n",
    "            token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "            attn_masks = encoded_pair['attention_mask'].squeeze(0)\n",
    "            if self.with_labels:  # True if the dataset has labels\n",
    "                label = self.data.loc[index, 'label']\n",
    "                return token_ids, attn_masks,  label  \n",
    "            else:\n",
    "                return token_ids, attn_masks\n",
    "        else:\n",
    "            token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "            attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "            token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label = self.data.loc[index, 'label']\n",
    "            return token_ids, attn_masks, token_type_ids, label  \n",
    "        else:\n",
    "            return token_ids, attn_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74516dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "\n",
    "def evaluate_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4074209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our metric\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            \n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            pred = torch.argmax(logits,dim=-1)\n",
    "            \n",
    "            right += torch.sum(pred==torch.argmax(labels,dim=-1)).item()                 #number of correct predictions in the batch\n",
    "            count += seq.size()[0]                         #batch size\n",
    "\n",
    "    return right / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d26c3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set some useful variables\n",
    "device = torch.device(\"cuda:0\")\n",
    "bert_model = \"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n",
    "maxlen = 216  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
    "bs = 64  # batch size\n",
    "iters_to_accumulate = 1  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
    "lr = 2e-5  # learning rate\n",
    "epochs = 4  # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfa679",
   "metadata": {},
   "source": [
    "# The Bert models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e06bb",
   "metadata": {},
   "source": [
    "All code from here to the next header must be executed before running one of the Bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f35f1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(net, criterion, opti, lr, train_loader, val_loader, epochs):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
    "            opti.zero_grad()\n",
    "            # Convert to cuda \n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "    \n",
    "            \n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "\n",
    "           \n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model='models{}_fineTune_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0c02136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model,output_hidden_states = True)\n",
    "\n",
    "        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        # Freeze bert layers and only train the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classification layer\n",
    "        self.cls_layer = nn.Linear(hidden_size, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
    "        pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)['hidden_states'][-1][:,0,:]\n",
    "            \n",
    "\n",
    "        \n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f023d9",
   "metadata": {},
   "source": [
    "# Albert with fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9b5f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run this for training\n",
    "\n",
    "#  Set all seeds to make reproducible results\n",
    "set_seed(1)\n",
    "freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
    "# training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "\n",
    "#  training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
    "\n",
    "#sorry for this, but was never working when one GPU\n",
    "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
    "\n",
    "net = nn.DataParallel(net, device_ids = [0,1,2,3])\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "train_bert(net, criterion, opti, lr, train_loader, val_loader, epochs, iters_to_accumulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240818c",
   "metadata": {},
   "source": [
    "We can see that it overfits very quickly, with the training loss continuously decreasing while the validation loss reaches a minimum after 2 epochs and then increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23492cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [00:24<00:00,  4.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9123402878861394"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(net, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75d3fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7176128093158661"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(net, device, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6953163",
   "metadata": {},
   "source": [
    "Note : these figures are not for the best model but after 4 epochs, when it is already largely overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d9c73",
   "metadata": {},
   "source": [
    "Not so bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab440a",
   "metadata": {},
   "source": [
    "To run our best model use this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72dbbf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "set_seed(1)\n",
    "freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
    "# training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "\n",
    "#  training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
    "\n",
    "#sorry for this, but was never working when one GPU\n",
    "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
    "\n",
    "net = nn.DataParallel(net, device_ids = [0,1,2,3])\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "net.load_state_dict(torch.load('modelsalbert-base-v2_fineTune_lr_2e-05_val_loss_0.6161_ep_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c9f9737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  2.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86608442503639"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(net, device, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3aec",
   "metadata": {},
   "source": [
    "Impressive! And it is the smallest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73da8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check the number of trainable parameters\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dd43ee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11685891"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531b13f",
   "metadata": {},
   "source": [
    "# Albert without fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc8cc7dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▎                                                                  | 19/97 [07:32<30:18, 23.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19/97 of epoch 1 complete. Loss : 1.0994238978938053 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▌                                                  | 38/97 [14:50<22:47, 23.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 38/97 of epoch 1 complete. Loss : 0.9854506787500883 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▊                                  | 57/97 [22:11<15:39, 23.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 57/97 of epoch 1 complete. Loss : 0.9258737783682974 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████                  | 76/97 [29:29<08:07, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 76/97 of epoch 1 complete. Loss : 0.8104557457723116 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████▎ | 95/97 [36:43<00:47, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 95/97 of epoch 1 complete. Loss : 0.8107262975291202 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [37:23<00:00, 23.13s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [01:27<00:00,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 complete! Validation Loss : 0.7098227522589944\n",
      "Best validation loss improved from inf to 0.7098227522589944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▎                                                                  | 19/97 [06:34<26:28, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19/97 of epoch 2 complete. Loss : 0.7501355817443446 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▌                                                  | 38/97 [13:30<21:32, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 38/97 of epoch 2 complete. Loss : 0.6424448207805031 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▊                                  | 57/97 [20:22<14:38, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 57/97 of epoch 2 complete. Loss : 0.6007815677868692 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████                  | 76/97 [27:30<07:47, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 76/97 of epoch 2 complete. Loss : 0.5194309824391415 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████▎ | 95/97 [34:38<00:45, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 95/97 of epoch 2 complete. Loss : 0.564765000029614 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [35:14<00:00, 21.80s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [01:25<00:00,  7.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 complete! Validation Loss : 0.6291261315345764\n",
      "Best validation loss improved from 0.7098227522589944 to 0.6291261315345764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▎                                                                  | 19/97 [06:35<27:00, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19/97 of epoch 3 complete. Loss : 0.48355331703236226 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▌                                                  | 38/97 [13:32<21:23, 21.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 38/97 of epoch 3 complete. Loss : 0.4110707248512067 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▊                                  | 57/97 [20:33<14:19, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 57/97 of epoch 3 complete. Loss : 0.3717924395674153 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████                  | 76/97 [27:32<07:32, 21.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 76/97 of epoch 3 complete. Loss : 0.32454180795895426 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████▎ | 95/97 [34:24<00:42, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 95/97 of epoch 3 complete. Loss : 0.3859226687958366 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [35:02<00:00, 21.68s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [01:26<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 complete! Validation Loss : 0.676251389763572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▎                                                                  | 19/97 [06:47<27:31, 21.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19/97 of epoch 4 complete. Loss : 0.3233075690896888 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▌                                                  | 38/97 [13:38<21:32, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 38/97 of epoch 4 complete. Loss : 0.2620254504053216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▊                                  | 57/97 [20:19<13:55, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 57/97 of epoch 4 complete. Loss : 0.26093638570685135 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████                  | 76/97 [27:18<07:48, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 76/97 of epoch 4 complete. Loss : 0.30649037345459584 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████▎ | 95/97 [34:19<00:44, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 95/97 of epoch 4 complete. Loss : 0.3368364352928965 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [34:58<00:00, 21.63s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [01:31<00:00,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 complete! Validation Loss : 0.789167192849246\n",
      "The model has been saved in modelsalbert-base-v2_lr_2e-05_val_loss_0.62913_ep_2.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "freezebert=True\n",
    "net2 = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
    "\n",
    "net2.to(device)\n",
    "\n",
    "#  Set all seeds to make reproducible results\n",
    "set_seed(1)\n",
    "\n",
    "# Creating instances of training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "# Creating instances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = optim.Adam(net2.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "train_bert(net2, criterion, opti, lr, train_loader, val_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3002791",
   "metadata": {},
   "source": [
    "Ridicuously slow on the CPU, but all GPU were busy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4fb744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the best model that we have trained without fine-tuning\n",
    "model2 = SentencePairClassifier(freeze_bert=True)\n",
    "model2.load_state_dict(torch.load('modelsalbert-base-v2_lr_2e-05_val_loss_0.62913_ep_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40502a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [13:17<00:00,  8.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8272683163512858"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model2,'cpu', train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f858052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [01:30<00:00,  8.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8326055312954876"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model2,'cpu', val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab0445",
   "metadata": {},
   "source": [
    "That is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf8430ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check the number of trainable parameters\n",
    "\n",
    "model2_parameters = filter(lambda p: p.requires_grad, model2.parameters())\n",
    "params2 = sum([np.prod(p.size()) for p in model2_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f0fbd83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2307"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a69d23",
   "metadata": {},
   "source": [
    "We have a look at examples that it is failing to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2213dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_load_all=DataLoader(val_set, batch_size=687, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a121f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(val_load_all)):\n",
    "        pred = model2(seq,attn_masks,token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8dc91ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predic=torch.argmax(pred,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aff02d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['pred']=predic.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8202fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['true'] = df_val['label'].apply(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10e95453",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predic=df_val[df_val['true']!=df_val['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0d4fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1357118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3079</td>\n",
       "      <td>No, I don't know.</td>\n",
       "      <td>I don't know what she said, no.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6992</td>\n",
       "      <td>According to the Natural Resources Conservation Service, this single, voluntary program will provide flexible technical, financial, and educational assistance to farmers and ranchers who face serious threats to soil, water, and related natural resources on agricultural and other lands, including grazing lands, wetlands, forest lands, and wildlife habitats.</td>\n",
       "      <td>Farmers and ranchers must have all of their licenses and permits to qualify.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>243</td>\n",
       "      <td>well they're so close to an undefeated undefeated season they can taste it and they wanna make history so i don't think they're gonna lack for motivation</td>\n",
       "      <td>Unless they suffer any losses, they'll remain motivated.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11207</td>\n",
       "      <td>Since there is no airport on the island, all visitors must arrive at the port, Skala, where most of the hotels are located and all commercial activity is carried out.</td>\n",
       "      <td>The best way to get onto the island is by plane.</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6088</td>\n",
       "      <td>Yes, it does, admitted Tuppence.</td>\n",
       "      <td>Tuppence wasn't very happy about admitting it did.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>5141</td>\n",
       "      <td>and the same is true of the drug hangover you know if you</td>\n",
       "      <td>It's just like a drug hangover but worse.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>4706</td>\n",
       "      <td>Sit down, will you?\" Tuppence sat down on the chair facing him.</td>\n",
       "      <td>He asked Tuppence to sit on a red chair.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>11686</td>\n",
       "      <td>Under Deng Xiaoping, Beijing actively sought to cultivate a good bilateral relationship.</td>\n",
       "      <td>Beijing sought to create a good relationship with Hong Kong.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>3336</td>\n",
       "      <td>The sacred is not mysterious to her.</td>\n",
       "      <td>The woman does not know anything sacred.</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>7171</td>\n",
       "      <td>One thing was worrying me dreadfully, but my heart gave a great throb of relief when I saw my ulster lying carelessly over the back of a chair.</td>\n",
       "      <td>The chair was tall and made of wood.</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                                                                                                                                                                                                                                                                                                                                                 premise                                                                     hypothesis      label  pred  true\n",
       "4     3079                                                                                                                                                                                                                                                                                                                                                      No, I don't know.                                                 I don't know what she said, no.  [0, 1, 0]     0     1\n",
       "11    6992  According to the Natural Resources Conservation Service, this single, voluntary program will provide flexible technical, financial, and educational assistance to farmers and ranchers who face serious threats to soil, water, and related natural resources on agricultural and other lands, including grazing lands, wetlands, forest lands, and wildlife habitats.  Farmers and ranchers must have all of their licenses and permits to qualify.   [0, 1, 0]     2     1\n",
       "12     243                                                                                                                                                                                                               well they're so close to an undefeated undefeated season they can taste it and they wanna make history so i don't think they're gonna lack for motivation                       Unless they suffer any losses, they'll remain motivated.  [0, 1, 0]     2     1\n",
       "21   11207                                                                                                                                                                                                  Since there is no airport on the island, all visitors must arrive at the port, Skala, where most of the hotels are located and all commercial activity is carried out.                               The best way to get onto the island is by plane.  [0, 0, 1]     1     2\n",
       "27    6088                                                                                                                                                                                                                                                                                                                                        Yes, it does, admitted Tuppence.                             Tuppence wasn't very happy about admitting it did.  [0, 1, 0]     2     1\n",
       "..     ...                                                                                                                                                                                                                                                                                                                                                                     ...                                                                            ...        ...   ...   ...\n",
       "647   5141                                                                                                                                                                                                                                                                                                               and the same is true of the drug hangover you know if you                                      It's just like a drug hangover but worse.  [0, 1, 0]     2     1\n",
       "651   4706                                                                                                                                                                                                                                                                                                         Sit down, will you?\" Tuppence sat down on the chair facing him.                                      He asked Tuppence to sit on a red chair.   [0, 1, 0]     2     1\n",
       "668  11686                                                                                                                                                                                                                                                                                Under Deng Xiaoping, Beijing actively sought to cultivate a good bilateral relationship.                   Beijing sought to create a good relationship with Hong Kong.  [0, 1, 0]     2     1\n",
       "669   3336                                                                                                                                                                                                                                                                                                                                    The sacred is not mysterious to her.                                       The woman does not know anything sacred.  [0, 0, 1]     0     2\n",
       "672   7171                                                                                                                                                                                                                         One thing was worrying me dreadfully, but my heart gave a great throb of relief when I saw my ulster lying carelessly over the back of a chair.                                          The chair was tall and made of wood.   [0, 1, 0]     2     1\n",
       "\n",
       "[115 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d9232",
   "metadata": {},
   "source": [
    "# Albert + URN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9753849",
   "metadata": {},
   "source": [
    "Base model: the (Al)Bert embedding for each word is projected in k(=105) dimensions, before the sentence is coded through the multiplication of the corresponding orthogonal matrices. The coding of both sentences are then concatenated. One or several linear layers classify the output. (Plus: simple ; Minus: what it is learning is basically just another way of encoding a sentence from the same tokens encoding. I am not sure the nice properties of the URN are anyway involved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d757e3b",
   "metadata": {},
   "source": [
    "In addition the data section, all the cells below should be executed before training or importing any of the Albert+URN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b79b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b74723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class UnitaryRNN(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        n = math.ceil(.5+math.sqrt(2*self.embedding_size+.25))   #we need n(n-1)/2 >= embedding_size\n",
    "        self.n = n                                               #in fact it is now implemented in such a way that we need\n",
    "                                                                 #the expression in paranthesis to be an integer\n",
    "        # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(n,n).long()\n",
    "        for i in range(0,n):\n",
    "            for j in range(i+1,n):\n",
    "                self.ix_mat[i,j] = (i* (2*n - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        device = text.device\n",
    "        x = torch.cat([torch.zeros(text.shape[:-1]).to(device).unsqueeze(-1), text], dim=-1)\n",
    "        tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape((*text.shape[:-1],self.n,self.n))\n",
    "        tri = tri - tri.transpose(-2, -1)\n",
    "        exp_mat = torch.matrix_exp(tri)\n",
    "        \n",
    "        h = torch.zeros((text.shape[0],self.n)).to(device)\n",
    "        h[:,0] = 1\n",
    "        #print('h_0',h.size())\n",
    "        for i in range(text.shape[1]):\n",
    "            h = torch.einsum('bij,bj->bi',exp_mat[:,i,:,:], h)  #batch matrices multiplication\n",
    "            #print('h',h.size())\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac84d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "def train(net, criterion, opti, lr, train_loader, val_loader, epochs,device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "    \n",
    "            \n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "\n",
    "            # Computing loss\n",
    "            \n",
    "            loss = criterion(logits.view((labels.size()[0],-1)), labels.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "                        \n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model='models{}_Alb_URN_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e84fed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            mean_loss += criterion(logits.view((labels.size()[0],-1)), labels.float()).item()\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0ce8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URN_Albert_base_NL(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False,embedding_size=105):\n",
    "        super(URN_Albert_base_NL, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model,output_hidden_states = True)\n",
    "\n",
    "        \n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        # Freeze bert layers and only train the projection and the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        #URN layer\n",
    "        self.urn_layer = UnitaryRNN(self.embedding_size)\n",
    "        \n",
    "        #sigmoid\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        #Projection layers from dim 768 to 105\n",
    "        self.lin1 = nn.Linear(768, 64)\n",
    "        self.proj = nn.Linear(64,105)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.cls_layer = nn.Linear(30, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision (a trick to try and spare time)\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "        device = input_ids.device\n",
    "        bs = input_ids.size()[0]  #batch size\n",
    "        end_sentence1 = torch.argmax(token_type_ids,dim=1)-2 #indice of the last word of the first sentence(that is not CLS)\n",
    "        #print(end_sentence1)\n",
    "        end_sentence2 = -torch.argmax(torch.flip(token_type_ids,dims=(1,)),dim=1)+214 #trick to get the last one\n",
    "        pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)['hidden_states'][-1]\n",
    "        lis_sent1= []\n",
    "        lis_sent2= []\n",
    "        for i in range(bs):          #loop on the batch for slicing and padding\n",
    "            sent1 = pooler_output[i,1:end_sentence1[i],:] #get the sentence 1 without separators Bert encoding of each word in 768 dim\n",
    "            sent1_padded = torch.cat((sent1,torch.zeros((220-sent1.size()[0],sent1.size()[1]),device=device)),dim=0)\n",
    "            sent2 = pooler_output[i,(end_sentence1[i]+3):end_sentence2[i],:]\n",
    "            sent2_padded = torch.cat((sent2,torch.zeros((64-sent2.size()[0],sent2.size()[1]),device=device)),dim=0)\n",
    "            lis_sent1.append(sent1_padded)\n",
    "            lis_sent2.append(sent2_padded)\n",
    "        \n",
    "        sent1 = torch.stack(lis_sent1)  #batch of padded Bert encoded sentences\n",
    "        sent2 = torch.stack(lis_sent2)\n",
    "        # for the URN layer to work on batches we need to pad with 0s sent1 and sent2\n",
    "        sent1 = self.sigmoid(self.lin1(sent1))\n",
    "        sent1_red = self.proj(sent1)  #size (batch_size, 220, 105)\n",
    "        #sent1_red = torch.cat((sent1_red,torch.zeros((bs,220-sent1_red.size()[1],sent1_red.size()[2]),device=device)),dim=1) #we pad to the longest premisse 196 tokens\n",
    "        sent2 = self.sigmoid(self.lin1(sent2))\n",
    "        sent2_red = self.proj(sent2)  #size (batch_size, 64, 105)\n",
    "        \n",
    "        sent1_red =self.urn_layer(sent1_red)   #size (batch_size, 15)\n",
    "        sent2_red =self.urn_layer(sent2_red)\n",
    "        \n",
    "        pooler_output = torch.cat((sent1_red,sent2_red),dim=1)  #concatenate the outputs of the URN\n",
    "        \n",
    "             \n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce557a17",
   "metadata": {},
   "source": [
    "# Albert + URN naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89258ebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run this to train a new model\n",
    "device = torch.device(\"cuda:0\")\n",
    "#  Set all seeds to make reproducible results\n",
    "set_seed(1)\n",
    "\n",
    "# Creating instances of training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "\n",
    "# Creating instances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=64, num_workers=4)\n",
    "\n",
    "net11 = URN_Albert_base_NL()\n",
    "\n",
    "net11 = nn.DataParallel(net11, device_ids = [0,1,2,3])\n",
    "\n",
    "net11.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = optim.Adam(net11.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
    "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
    "\n",
    "\n",
    "train(net11, criterion, opti, lr, train_loader, val_loader, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a98030",
   "metadata": {},
   "source": [
    "Some learning happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d87e7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this to load the best model\n",
    "device = torch.device(\"cuda:0\")\n",
    "#  Set all seeds to make reproducible results\n",
    "set_seed(1)\n",
    "\n",
    "# Creating instances of training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "\n",
    "# Creating instances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=64, num_workers=4)\n",
    "\n",
    "net11 = URN_Albert_base_NL()\n",
    "\n",
    "net11 = nn.DataParallel(net11, device_ids = [0,1,2,3])\n",
    "\n",
    "net11.to(device)\n",
    "net11.load_state_dict(torch.load('modelsalbert-base-v2_Alb_URN_val_loss_2e-05_ep_0.7852.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5129c28",
   "metadata": {},
   "source": [
    "# The Model Albert+URN refined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecadd5f",
   "metadata": {},
   "source": [
    "(one of) the problems of training the URN on such a database is that numerous words will never be encountered in the training set. Nevertheless we would like our model to use the cosine similarity of these new worlds with some known ones to generalise the heuristic it may have learned. For example, if the model has learned that in the pair : \"All birds fly. Some bird fly.\" the second is consequence of the first, we want it to identify the new hypothesis : \"all ravens fly.\" as a rightful consequence of the same premisse.\n",
    "One way to do this would be to encode all words of the consequence but one in orthogonal matrices. so that the encoded sentence would become $O_1,..O_{k-1},w_k,O_{k+1}...O_n$ where O_i are orthogonal matrices, w_k is a vector, n is the number of words in the sentence and k is the position of the untouched (not transformed in an ortho matrix) word in the sentence. Then, to benefit from the property that 2 vectors v and v' $<\\prod_{i=k+1}^{n}O_i v,\\prod_{i=k+1}^{n}O_i v'> = <v,v'>$ (In reality we will need $t(\\prod_{i=k+1}^{n}O_i v)t(\\prod_{i=1}^{k-1}O_i)$ to process each word both by the left and the right side of the sentence, but the idea stays the same) w_k needs to be projected onto a space of the same dimension as the orthogonal matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f26c7e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: batch of sentences (words embedded in 105 dim - text) + batch of of sentences (with words embedded in 15 dim - h)\n",
    "#output:for each word (in position i) in the sentence, returns $\\Pi_{i+1}^n O_i v_i$. In words, the ith word is 'processed through\n",
    "#the right side of the sentence.\n",
    "\n",
    "import math\n",
    "class UnitaryRNNright(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        n = math.ceil(.5+math.sqrt(2*self.embedding_size+.25))   #we need n(n-1)/2 >= embedding_size\n",
    "        self.n = n                                               #in fact it is now implemented in such a way that we need\n",
    "                                                                 #the expression in paranthesis to be an integer\n",
    "        # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(n,n).long()\n",
    "        for i in range(0,n):\n",
    "            for j in range(i+1,n):\n",
    "                self.ix_mat[i,j] = (i* (2*n - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "    def forward(self, text, h):\n",
    "\n",
    "        device = text.device\n",
    "        x = torch.cat([torch.zeros(text.shape[:-1]).to(device).unsqueeze(-1), text], dim=-1)\n",
    "        tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape((*text.shape[:-1],self.n,self.n))\n",
    "        tri = tri - tri.transpose(-2, -1)\n",
    "        exp_mat = torch.matrix_exp(tri)\n",
    "        list_results=[]\n",
    "        for i in range (text.shape[1]):\n",
    "            h0 = torch.clone(h[:,i,:]).detach()\n",
    "            for j in range(i+1,text.shape[1]):\n",
    "                h0 = torch.einsum('bij,bj->bi',exp_mat[:,j,:,:], h0)  #batch matrices multiplication\n",
    "                #print('h',h.size())\n",
    "            list_results.append(h0)\n",
    "            \n",
    "        return torch.stack(list_results,dim=1)  #(batch_size,sentence_len, 15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea5d4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: batch of sentences (words embedded in 105 dim - text) + batch of of sentences (with words embedded in 15 dim - h)\n",
    "#output:for each word (in position i) in the sentence, returns $\\t(v_i)t(Pi_0^{i-1}) O_i$. In words, the ith word is 'processed through\n",
    "#the leftt side of the sentence.\n",
    "\n",
    "import math\n",
    "class UnitaryRNNleft(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        n = math.ceil(.5+math.sqrt(2*self.embedding_size+.25))   #we need n(n-1)/2 >= embedding_size\n",
    "        self.n = n                                               #in fact it is now implemented in such a way that we need\n",
    "                                                                 #the expression in paranthesis to be an integer\n",
    "        # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(n,n).long()\n",
    "        for i in range(0,n):\n",
    "            for j in range(i+1,n):\n",
    "                self.ix_mat[i,j] = (i* (2*n - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "    def forward(self, text, h):\n",
    "\n",
    "        device = text.device\n",
    "        x = torch.cat([torch.zeros(text.shape[:-1]).to(device).unsqueeze(-1), text], dim=-1)\n",
    "        tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape((*text.shape[:-1],self.n,self.n))\n",
    "        tri = tri - tri.transpose(-2, -1)\n",
    "        exp_mat = torch.matrix_exp(tri)\n",
    "        list_results=[]\n",
    "        for i in range (text.shape[1]):\n",
    "            h0 = torch.clone(h[:,i,:]).detach()\n",
    "            for j in range(i-1,-1,-1):\n",
    "                h0 = torch.einsum('bij,bj->bi',exp_mat[:,j,:,:], h0)  #batch matrices multiplication\n",
    "                    #in fact we retrun the transposed vectors, which is the same \n",
    "            list_results.append(h0)\n",
    "            \n",
    "        return torch.stack(list_results,dim=1)  #(batch_size,sentence_len, 15)\n",
    "                                                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e16a9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A first try that fails to learn\n",
    "class AlbURN_NL(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False,embedding_size=105):\n",
    "        super(AlbURN_NL, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model,output_hidden_states = True)\n",
    "\n",
    "        \n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        # Freeze bert layers and only train the projection and the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        #URN layer\n",
    "        self.urn_right = UnitaryRNNright(self.embedding_size)\n",
    "        self.urn_left = UnitaryRNNleft(self.embedding_size)\n",
    "        \n",
    "        #sigmoid\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        #Projection layer from dim 768 to 105\n",
    "        self.lin1 = nn.Linear(768, 64)\n",
    "        \n",
    "        \n",
    "        #Projection layer from dim 768 to 105\n",
    "        \n",
    "        self.proj1 = nn.Linear(64,105) #get the text input\n",
    "        \n",
    "        self.proj2 = nn.Linear(64,15) #get the h input for the right URN\n",
    "        \n",
    "        # Classification layer\n",
    "        # this time we need to classify based on a (batch_size, sent_len, 15) tensor\n",
    "        # our hope is that at least one of the sent_len vectors is very informative in relation to the classification result\n",
    "        # but we don't know which one... Maybe here an attention layer could answer the question? (next model)\n",
    "        # for now: flatten + linear\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cls_layer = nn.Linear(284*15, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "        device = input_ids.device\n",
    "        bs = input_ids.size()[0]  #batch size\n",
    "        end_sentence1 = torch.argmax(token_type_ids,dim=1)-2 #indice of the last word of the first sentence(that is not CLS)\n",
    "        #print(end_sentence1)\n",
    "        end_sentence2 = -torch.argmax(torch.flip(token_type_ids,dims=(1,)),dim=1)+214 #trick to get the last one\n",
    "        pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)['hidden_states'][-1]\n",
    "        lis_sent1= []\n",
    "        lis_sent2= []\n",
    "        for i in range(bs):          #loop on the batch for slicing and padding\n",
    "            sent1 = pooler_output[i,1:end_sentence1[i],:] #get the sentence 1 without separators Bert encoding of each word in 768 dim\n",
    "            sent1_padded = torch.cat((sent1,torch.zeros((220-sent1.size()[0],sent1.size()[1]),device=device)),dim=0)\n",
    "            sent2 = pooler_output[i,(end_sentence1[i]+3):end_sentence2[i],:]\n",
    "            sent2_padded = torch.cat((sent2,torch.zeros((64-sent2.size()[0],sent2.size()[1]),device=device)),dim=0)\n",
    "            lis_sent1.append(sent1_padded)\n",
    "            lis_sent2.append(sent2_padded)\n",
    "        \n",
    "        sent1 = torch.stack(lis_sent1)  #batch of padded Bert encoded sentences\n",
    "        sent2 = torch.stack(lis_sent2)\n",
    "        # for the URN layer to work on batches we need to pad with 0s sent1 and sent2\n",
    "        sent1 = self.lin1(sent1)\n",
    "        text1 = self.sigmoid(sent1)\n",
    "        text1 = self.proj1(text1)  #size (batch_size, 220, 105)\n",
    "        #sent1_red = torch.cat((sent1_red,torch.zeros((bs,220-sent1_red.size()[1],sent1_red.size()[2]),device=device)),dim=1) #we pad to the longest premisse 196 tokens\n",
    "        sent2 = self.lin1(sent2)\n",
    "        text2 = self.sigmoid(sent2)\n",
    "        text2 = self.proj1(text2)  #size (batch_size, 64, 105)\n",
    "        #sent2_red = torch.cat((sent2_red,torch.zeros((bs,64-sent2_red.size()[1],sent2_red.size()[2]),device=device)),dim=1)\n",
    "        h1 = self.proj2(sent1)  #(bs,220,15)\n",
    "        h2 = self.proj2(sent2)  #(bs,64,15)\n",
    "        \n",
    "        sent1_right_proc =self.urn_right(text1, h1)   #size (batch_size, 15)\n",
    "        #print('sent1_red',sent1_red.size())\n",
    "        sent2_right_proc =self.urn_right(text2, h2)\n",
    "        sent1_proc = self.urn_left(text1, sent1_right_proc) #(bs, 220, 15)\n",
    "        sent2_proc = self.urn_left(text2, sent2_right_proc) #(bs, 64, 15)\n",
    "        pooler_output = torch.cat((sent1_proc,sent2_proc),dim=1)  #concatenate the outputs of the URN (bs, 284,15)\n",
    "        \n",
    "        pooler_output = self.flatten(pooler_output)\n",
    "        #print('pooler_out',pooler_output.size())\n",
    "\n",
    "        \n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8cc10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second version - we disconnect the learning for h and for the orthogonal matrices\n",
    "#This one learns but slowly\n",
    "class AlbURN_NL_sep(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False,embedding_size=105):\n",
    "        super(AlbURN_NL_sep, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model,output_hidden_states = True)\n",
    "\n",
    "        \n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        # Freeze bert layers and only train the projection and the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        #URN layer\n",
    "        self.urn_right = UnitaryRNNright(self.embedding_size)\n",
    "        self.urn_left = UnitaryRNNleft(self.embedding_size)\n",
    "        \n",
    "        #sigmoid\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        #Projection layer from dim 768 to 105\n",
    "        self.lin1 = nn.Linear(768, 64)\n",
    "        \n",
    "        \n",
    "        #Projection layer from dim 768 to 105\n",
    "        \n",
    "        self.proj1 = nn.Linear(64,105) #get the text input\n",
    "        \n",
    "        self.proj2 = nn.Linear(768,15) #get the h input for the right URN directly out of Bert\n",
    "        \n",
    "        # Classification layer\n",
    "        # this time we need to classify based on a (batch_size, sent_len, 15) tensor\n",
    "        # our hope is that at least one of the sent_len vectors is very informative in relation to the classification result\n",
    "        # but we don't know which one... Maybe here an attention layer could answer the question? (next model)\n",
    "        # for now: flatten + linear\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cls_layer = nn.Linear(284*15, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "        device = input_ids.device\n",
    "        bs = input_ids.size()[0]  #batch size\n",
    "        end_sentence1 = torch.argmax(token_type_ids,dim=1)-2 #indice of the last word of the first sentence(that is not CLS)\n",
    "        #print(end_sentence1)\n",
    "        end_sentence2 = -torch.argmax(torch.flip(token_type_ids,dims=(1,)),dim=1)+214 #trick to get the last one\n",
    "        pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)['hidden_states'][-1]\n",
    "        lis_sent1= []\n",
    "        lis_sent2= []\n",
    "        for i in range(bs):          #loop on the batch for slicing and padding\n",
    "            sent1 = pooler_output[i,1:end_sentence1[i],:] #get the sentence 1 without separators Bert encoding of each word in 768 dim\n",
    "            sent1_padded = torch.cat((sent1,torch.zeros((220-sent1.size()[0],sent1.size()[1]),device=device)),dim=0)\n",
    "            sent2 = pooler_output[i,(end_sentence1[i]+3):end_sentence2[i],:]\n",
    "            sent2_padded = torch.cat((sent2,torch.zeros((64-sent2.size()[0],sent2.size()[1]),device=device)),dim=0)\n",
    "            lis_sent1.append(sent1_padded)\n",
    "            lis_sent2.append(sent2_padded)\n",
    "        \n",
    "        sent1 = torch.stack(lis_sent1)  #batch of padded Bert encoded sentences\n",
    "        sent2 = torch.stack(lis_sent2)\n",
    "        # for the URN layer to work on batches we need to pad with 0s sent1 and sent2\n",
    "        text1 = self.lin1(sent1)\n",
    "        text1 = self.sigmoid(text1)\n",
    "        text1 = self.proj1(text1)  #size (batch_size, 220, 105)\n",
    "        #sent1_red = torch.cat((sent1_red,torch.zeros((bs,196-sent1_red.size()[1],sent1_red.size()[2]),device=device)),dim=1) #we pad to the longest premisse 196 tokens\n",
    "        text2 = self.lin1(sent2)\n",
    "        text2 = self.sigmoid(text2)\n",
    "        text2 = self.proj1(text2)  #size (batch_size, 64, 105)\n",
    "        #sent2_red = torch.cat((sent2_red,torch.zeros((bs,64-sent2_red.size()[1],sent2_red.size()[2]),device=device)),dim=1)\n",
    "        h1 = self.proj2(sent1)  #(bs,220,15)\n",
    "        h2 = self.proj2(sent2)  #(bs,64,15)\n",
    "        \n",
    "        sent1_right_proc =self.urn_right(text1, h1)   #size (batch_size, 15)\n",
    "        \n",
    "        sent2_right_proc =self.urn_right(text2, h2)\n",
    "        sent1_proc = self.urn_left(text1, sent1_right_proc) #(bs, 220, 15)\n",
    "        sent2_proc = self.urn_left(text2, sent2_right_proc) #(bs, 64, 15)\n",
    "        pooler_output = torch.cat((sent1_proc,sent2_proc),dim=1)  #concatenate the outputs of the URN (bs, 284,15)\n",
    "        \n",
    "        pooler_output = self.flatten(pooler_output)\n",
    "       \n",
    "\n",
    "        \n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c968499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "def train(net, criterion, opti, lr, train_loader, val_loader, epochs,device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scaler = GradScaler() #necessary with autograder\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "    \n",
    "            # Enables autocasting for the forward pass (model + loss)\n",
    "            #with autocast():\n",
    "                # Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "\n",
    "            # Computing loss\n",
    "            \n",
    "            loss = criterion(logits.view((labels.size()[0],-1)), labels.float())\n",
    "            #loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                    net_copy = copy.deepcopy(net)\n",
    "                    path_to_model='models{}_Alb_URN_NL_ep_0.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                    torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model='models{}_Alb_URN_NL_val_loss_{}_ep_{}.pt'.format(net.__class__.__name__, best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddafdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            mean_loss += criterion(logits.view((labels.size()[0],-1)), labels.float()).item()\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507af841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 19%|███████████████▍                                                                   | 18/97 [09:04<41:30, 31.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 19/97 of epoch 1 complete. Loss : 1.077095433285362 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▎                                                                  | 19/97 [09:31<39:14, 30.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been saved in modelsDataParallel_Alb_URN_NL_ep_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▌                                                  | 38/97 [18:32<27:28, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 38/97 of epoch 1 complete. Loss : 1.0777917159231085 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▊                                  | 57/97 [27:37<19:03, 28.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 57/97 of epoch 1 complete. Loss : 1.089080810546875 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████                  | 76/97 [36:44<10:06, 28.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 76/97 of epoch 1 complete. Loss : 1.0957629555150081 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████▎ | 95/97 [45:59<00:58, 29.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 95/97 of epoch 1 complete. Loss : 1.0963227121453536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 97/97 [46:56<00:00, 29.03s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [02:01<00:00, 11.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 complete! Validation Loss : 1.0828220952640881\n",
      "Best validation loss improved from inf to 1.0828220952640881\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████▍                                                                   | 18/97 [10:15<52:46, 40.08s/it]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#  Set all seeds to make reproducible results\n",
    "set_seed(1)\n",
    "\n",
    "# Creating instances of training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "\n",
    "# Creating instances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=64, num_workers=4)\n",
    "\n",
    "net13 = AlbURN_NL_sep()\n",
    "\n",
    "net13 = nn.DataParallel(net13, device_ids = [0,1,2,3])\n",
    "\n",
    "net13.to(device)\n",
    "net13.load_state_dict(torch.load('modelsDataParallel_Alb_URN_NL_ep_0.pt'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = optim.Adam(net13.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
    "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
    "\n",
    "\n",
    "train_l,val_l = train(net13, criterion, opti, lr, train_loader, val_loader, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c57972",
   "metadata": {},
   "source": [
    "Some learning is taking place, but it is very slow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
